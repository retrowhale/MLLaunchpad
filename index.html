<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Test Machine Learning Launchpad</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Header */
        header {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 1000;
            border-bottom: 1px solid rgba(255, 255, 255, 0.2);
        }

        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
        }

        .logo {
            font-size: 1.8rem;
            font-weight: bold;
            color: white;
            text-decoration: none;
        }

        .nav-links {
            display: flex;
            list-style: none;
            gap: 0.5rem; /* Adjusted for more links */
            flex-wrap: wrap;
            align-items: center;
            justify-content: center;
        }

        .nav-links a {
            color: white;
            text-decoration: none;
            transition: color 0.3s ease;
            padding: 0.5rem 0.8rem;
            border-radius: 5px;
            cursor: pointer;
            font-size: 0.9rem;
        }

        .nav-links a:hover, .nav-links a.active {
            background: rgba(255, 255, 255, 0.2);
            color: #fff;
        }

        /* Main Content */
        main {
            background: white;
            margin: 2rem 0;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        .page {
            display: none;
            padding: 3rem;
            animation: fadeIn 0.5s ease-in;
        }

        .page.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        h1 {
            color: #2c3e50;
            font-size: 2.5rem;
            margin-bottom: 2rem;
            text-align: center;
            background: linear-gradient(45deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        h2 {
            color: #34495e;
            font-size: 1.8rem;
            margin: 2rem 0 1rem 0;
            border-bottom: 2px solid #3498db;
            padding-bottom: 0.5rem;
        }

        h3 {
            color: #2c3e50;
            font-size: 1.4rem;
            margin: 1.5rem 0 1rem 0;
        }

        p {
            margin-bottom: 1rem;
            font-size: 1.1rem;
            line-height: 1.8;
        }

        /* Code Blocks */
        .code-block {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            position: relative;
        }

        .code-block::before {
            content: 'Python';
            position: absolute;
            top: 0.5rem;
            right: 1rem;
            font-size: 0.8rem;
            color: #95a5a6;
        }
        
        pre {
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        /* Buttons */
        .cta-button {
            background: linear-gradient(45deg, #3498db, #2980b9);
            color: white;
            padding: 1rem 2rem;
            border: none;
            border-radius: 30px;
            font-size: 1.1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            text-decoration: none;
            display: inline-block;
            margin: 1rem 0;
            box-shadow: 0 5px 15px rgba(52, 152, 219, 0.3);
        }

        .cta-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(52, 152, 219, 0.4);
        }

        /* Cards */
        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .card {
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
            border-left: 4px solid #3498db;
        }

        .card:hover {
            transform: translateY(-5px);
        }

        .card h3, .card h5 {
            color: #2c3e50;
            margin-bottom: 1rem;
        }

        /* Lists */
        ul {
            margin: 1rem 0;
            padding-left: 2rem;
        }

        li {
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        /* Highlight boxes */
        .highlight-box {
            background: linear-gradient(135deg, #74b9ff, #0984e3);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: 0 5px 15px rgba(116, 185, 255, 0.3);
        }

        .warning-box {
            background: linear-gradient(135deg, #fdcb6e, #e17055);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: 0 5px 15px rgba(253, 203, 110, 0.3);
        }
        
        /* Accordion for solutions */
        .accordion-button {
            background-color: #e7f1ff;
            color: #0d6efd;
            cursor: pointer;
            padding: 18px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 1.2rem;
            transition: 0.4s;
            border-radius: 8px;
            margin-top: 1rem;
            font-weight: bold;
        }

        .accordion-button:hover {
            background-color: #d0e3ff;
        }

        .accordion-panel {
            padding: 0 18px;
            background-color: white;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.2s ease-out;
            border: 1px solid #e7f1ff;
            border-top: none;
            border-radius: 0 0 8px 8px;
        }

        /* Responsive */
        @media (max-width: 1100px) {
            .nav-links {
                gap: 0.2rem;
                width: 100%;
                margin-top: 1rem;
            }
             .nav-links a {
                padding: 0.4rem 0.6rem;
                font-size: 0.85rem;
            }
        }
        @media (max-width: 768px) {
            h1 {
                font-size: 2rem;
            }
            .page {
                padding: 2rem 1rem;
            }
        }
    </style>
</head>
<body>
    <header>
        <nav class="container">
            <a href="#" class="logo" onclick="showPage('home', this)">üöÄ ML Launchpad</a>
            <ul class="nav-links">
                <li><a href="#" onclick="showPage('home', this)" class="active">Home</a></li>
                <li><a href="#" onclick="showPage('python-basics', this)">Python</a></li>
                <li><a href="#" onclick="showPage('data-prep', this)">Data Prep</a></li>
                <li><a href="#" onclick="showPage('regression-exercise', this)">Regression</a></li>
                <li><a href="#" onclick="showPage('decision-trees', this)">Trees</a></li>
                <li><a href="#" onclick="showPage('knn', this)">k-NN</a></li>
                <li><a href="#" onclick="showPage('ensemble', this)">Ensemble</a></li>
                <li><a href="#" onclick="showPage('neural-networks', this)">NN/Keras</a></li>
                <li><a href="#" onclick="showPage('evaluation', this)">Evaluation</a></li>
                <li><a href="#" onclick="showPage('unsupervised', this)">Unsupervised</a></li>
                <li><a href="#" onclick="showPage('lab-exercise', this)" style="background: #e74c3c;">Churn Lab</a></li>
                <li><a href="#" onclick="showPage('advanced-exercise', this)" style="background: #e74c3c;">Adv. Exercises</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <!-- Page 1: Homepage -->
        <div id="home" class="page active">
            <h1>Welcome to the Machine Learning Launchpad</h1>
            <div class="highlight-box">
                <h2>üéØ Your Journey Starts Here</h2>
                <p>This comprehensive launchpad will take you from Python fundamentals to building advanced, intelligent models. Explore key concepts, work through practical examples, and master the skills needed for modern data science.</p>
            </div>
            <h2>üåü What You'll Learn</h2>
            <div class="card-grid">
                <div class="card"><h3>üêç Python Mastery</h3><p>Go from Python basics to advanced data manipulation with essential libraries like NumPy, Pandas, and Matplotlib.</p></div>
                <div class="card"><h3>üß† ML Foundations</h3><p>Understand the theory behind key algorithms, from simple regression to complex neural networks.</p></div>
                <div class="card"><h3>üîß Hands-On Projects</h3><p>Build real machine learning models with step-by-step guidance and runnable code examples.</p></div>
                <div class="card"><h3>üìä Advanced Techniques</h3><p>Dive into ensemble methods, deep learning, and robust model evaluation to build high-performance models.</p></div>
            </div>
            <div style="text-align: center; margin-top: 3rem;"><a href="#" onclick="showPage('python-basics', this)" class="cta-button">üöÄ Get Started with Python</a></div>
        </div>

        <!-- Page 2: Python Essentials -->
        <div id="python-basics" class="page">
            <h1>üêç Python Essentials for Machine Learning</h1>
            <p>A quick refresher on the Python fundamentals and the core data science libraries.</p>
            <h2>The Data Science Toolkit: Key Libraries</h2>
            <p>These are the essential libraries you will import at the start of almost every machine learning project.</p>
            <div class="code-block"><pre># Core libraries for data handling and deep learning
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-Learn for classical ML models and tools
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Imbalanced-Learn for handling imbalanced datasets
from imblearn.over_sampling import SMOTE</pre></div>

            <div class="card-grid">
                <div class="card"><h5>NumPy</h5><p>The fundamental package for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.</p></div>
                <div class="card"><h5>Pandas</h5><p>The primary tool for data manipulation and analysis. It introduces the <strong>DataFrame</strong>, a powerful and flexible data structure for handling tabular data (like spreadsheets or SQL tables).</p></div>
                <div class="card"><h5>Matplotlib & Seaborn</h5><p>The standard libraries for data visualization. <strong>Matplotlib</strong> provides the basic, low-level plotting framework, while <strong>Seaborn</strong> is built on top of it to create more attractive and informative statistical graphics.</p></div>
                <div class="card"><h5>TensorFlow & Keras</h5><p><strong>TensorFlow</strong> is a comprehensive platform for large-scale machine learning. <strong>Keras</strong> is its user-friendly, high-level API for building and training neural networks with ease.</p></div>
                <div class="card"><h5>Scikit-Learn (sklearn)</h5><p>The go-to library for classical machine learning. It provides a vast collection of tools for data preprocessing, a wide range of algorithms (like Logistic Regression, Decision Trees, Random Forests), and robust metrics for model evaluation.</p></div>
                <div class="card"><h5>Imbalanced-Learn (imblearn)</h5><p>A specialized library for dealing with imbalanced datasets. It offers techniques like <strong>SMOTE</strong> (Synthetic Minority Over-sampling Technique) to create synthetic samples of the minority class, helping to prevent models from becoming biased towards the majority class.</p></div>
            </div>
        </div>

        <!-- Page 3: Data Exploration and Preprocessing -->
        <div id="data-prep" class="page">
            <h1>Preparing Your Data for Machine Learning</h1>
            <p>Garbage in, garbage out. The quality of your data preparation directly determines the quality of your model. This is often the most time-consuming but critical phase of any ML project.</p>
            
            <h2>üîç Exploratory Data Analysis (EDA)</h2>
            <p>Before you preprocess, you must explore. EDA involves using visualizations and summary statistics to understand your data's structure, identify outliers, and discover underlying patterns. Tools like Matplotlib and Seaborn are essential for this.</p>
            
            <h2>üßπ Data Cleaning</h2>
            <p>Real-world data is messy. Cleaning involves handling issues like missing values and outliers that can negatively impact model performance.</p>
            <div class="code-block"><pre># Filling missing values in a column with its median
median_age = df['age'].median()
df['age'].fillna(median_age, inplace=True)</pre></div>

            <h2>‚öñÔ∏è Feature Scaling</h2>
            <p>Many algorithms, especially those based on distance (like k-NN and SVMs), are sensitive to the scale of features. If one feature (e.g., salary) has a much larger range than another (e.g., years of experience), it will dominate the distance calculations. Scaling brings all features to a similar magnitude.</p>
            <div class="code-block"><pre>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numerical_features)</pre></div>

            <h2>üè∑Ô∏è Encoding Categorical Data</h2>
            <p>Machine learning models require numerical input. Categorical features (like "Country" or "Color") must be converted into numbers. A common method is <strong>One-Hot Encoding</strong>, which creates a new binary column for each category.</p>
            <div class="code-block"><pre>from sklearn.preprocessing import OneHotEncoder

# This will create new columns like 'Color_Red', 'Color_Blue', etc.
encoder = OneHotEncoder()
X_categorical_encoded = encoder.fit_transform(X_categorical_features)</pre></div>

            <h2>üî™ Train/Test Split</h2>
            <div class="warning-box"><h3>üö® Critical Step: Train/Test Split</h3><p>We must evaluate our model on data it has never seen before to get an unbiased estimate of its real-world performance. This prevents <strong>overfitting</strong>, where a model memorizes the training data but fails to generalize to new data.</p></div>
            <div class="code-block"><pre>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</pre></div>
        </div>
        
        <!-- Page 4: Basic Regression Exercise -->
        <div id="regression-exercise" class="page">
            <h1>Exercise: Predicting Scores with Linear Regression</h1>
            <h2>1. Goal</h2>
            <p>You will build a model that predicts a student's exam score based on the number of hours they studied. This is a classic example of "error-based" supervised learning.</p>
            <h2>3. Step-by-Step Procedure</h2>
            <h3>Step 1: Import Libraries & Create Data</h3>
            <div class="code-block"><pre>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Create synthetic data
np.random.seed(0)
hours_studied = np.random.rand(50, 1) * 10
exam_score = 50 + 5 * hours_studied + np.random.randn(50, 1) * 5
df = pd.DataFrame({'Hours_Studied': hours_studied.flatten(), 'Exam_Score': exam_score.flatten()})</pre></div>
            <h3>Step 2: Define Features, Split Data, and Train Model</h3>
            <div class="code-block"><pre>X = df[['Hours_Studied']]
y = df['Exam_Score']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)</pre></div>
            <h3>Step 3: Evaluate the Model</h3>
            <div class="code-block"><pre>y_pred = model.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")</pre></div>
            <h2>4. Interpretation and Questions</h2>
            <ul>
                <li><strong>The Regression Line:</strong> What does the line of best fit represent?</li>
                <li><strong>Model Error:</strong> What does the RMSE value tell you about the model's average prediction error?</li>
            </ul>
        </div>

        <!-- Page 5: Decision Trees -->
        <div id="decision-trees" class="page">
            <h1>üå≥ Information-Based Learning: Decision Trees</h1>
            <p>Decision Trees make predictions by learning a series of if-then-else questions based on the features, forming a tree-like structure. They are highly interpretable and can be used for both classification and regression.</p>
            <h2>üçÉ How They Work: Entropy and Information Gain</h2>
            <p>The tree decides which feature to split on at each node by picking the one that provides the most "information gain." Information gain measures how much a split reduces the uncertainty (or "entropy") in the data. The goal is to create splits that result in "pure" nodes, where the data points in each node belong to a single class.</p>
            <div class="code-block"><pre>from sklearn.tree import DecisionTreeClassifier

# 'max_depth' is a key hyperparameter to prevent overfitting
tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_clf.fit(X_train, y_train)</pre></div>
        </div>
        
        <!-- Page 6: k-Nearest Neighbors (k-NN) -->
        <div id="knn" class="page">
            <h1>ü§ù Similarity-Based Learning: k-Nearest Neighbors (k-NN)</h1>
            <p>k-NN classifies a new data point based on the majority class of its 'k' closest neighbors in the feature space. It's simple, intuitive, and powerful.</p>
            <div class="warning-box"><h3>üö® The Importance of Feature Scaling</h3><p>k-NN relies on distance, so features with large ranges can dominate those with small ranges. It is <strong>critical</strong> to scale your data before applying k-NN using a tool like `StandardScaler`.</p></div>
            <div class="code-block"><pre>from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)</pre></div>
        </div>

        <!-- Page 7: Ensemble Methods -->
        <div id="ensemble" class="page">
            <h1>üå≥ Ensemble Methods: The Power of Many</h1>
            <h2>Boosting Explained</h2>
            <p>Boosting is an ensemble technique that combines multiple simple models (weak learners) to create a single, powerful predictive model. It works sequentially, with each new model focusing on correcting the errors made by the previous one. This iterative approach makes it highly effective.</p>
            <div class="card-grid">
                <div class="card">
                    <h5>AdaBoost (Adaptive Boosting)</h5>
                    <p>Focuses on misclassified instances by increasing their weights in subsequent iterations. This forces the next weak learner to pay more attention to the "hard" examples that the previous one got wrong.</p>
                </div>
                <div class="card">
                    <h5>Gradient Boosting</h5>
                    <p>A more generalized approach where each new model predicts the residuals (the errors) of the previous model. By continuously adding models that correct the previous errors, the overall model becomes highly accurate.</p>
                </div>
            </div>
             <h3>Hyperparameter Tuning</h3>
            <p>Boosting models have key hyperparameters like `n_estimators` (number of trees), `learning_rate` (how much each tree contributes), and `max_depth` (complexity of each tree). Using `GridSearchCV` helps find the optimal combination of these parameters to maximize performance.</p>
        </div>

        <!-- Page 8: Neural Networks -->
        <div id="neural-networks" class="page">
            <h1>üß† Deep Learning with Neural Networks</h1>
            <h2>Building a Neural Network with Keras</h2>
            <p>Neural networks, the core of deep learning, are inspired by the human brain. Building one with a framework like Keras involves stacking layers. Below is a simple architecture for image classification.</p>
            <h3>Key Layers in a CNN</h3>
            <ul>
                <li><strong>Conv2D:</strong> A convolutional layer that acts as a feature detector, scanning the image to find patterns like edges, corners, and textures.</li>
                <li><strong>MaxPooling2D:</strong> A pooling layer that reduces the spatial dimensions (down-sampling) of the feature map, making the model more efficient and robust to variations in the position of features.</li>
                <li><strong>Flatten:</strong> Converts the 2D feature maps into a 1D vector, preparing the data to be fed into the fully-connected dense layers.</li>
                <li><strong>Dense:</strong> A standard, fully-connected neural network layer where each neuron is connected to every neuron in the previous layer.</li>
                <li><strong>Activation Functions:</strong> 'relu' is commonly used in hidden layers to introduce non-linearity, while 'softmax' is used in the output layer for multi-class classification to convert outputs into probabilities.</li>
            </ul>
            <div class="code-block"><pre>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    keras.Input(shape=(32, 32, 3)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])</pre></div>
        </div>

        <!-- Page 9: Model Evaluation -->
        <div id="evaluation" class="page">
            <h1>üìä Robust Model Evaluation</h1>
            <p>A model is only as good as its evaluation. Choosing the right metrics is key to understanding its true performance, especially when dealing with real-world problems where errors have different costs.</p>
            
            <h2>The Confusion Matrix</h2>
            <p>This is the foundation for most classification metrics. It's a table that summarizes how a classification model performed by showing the counts of correct and incorrect predictions for each class.</p>
            <ul>
                <li><strong>True Positives (TP):</strong> The model correctly predicted the positive class. (e.g., correctly identified a fraudulent transaction).</li>
                <li><strong>True Negatives (TN):</strong> The model correctly predicted the negative class. (e.g., correctly identified a legitimate transaction).</li>
                <li><strong>False Positives (FP):</strong> The model incorrectly predicted the positive class. (e.g., flagged a legitimate transaction as fraud - a "false alarm").</li>
                <li><strong>False Negatives (FN):</strong> The model incorrectly predicted the negative class. (e.g., missed a fraudulent transaction - a very costly error!).</li>
            </ul>

            <div class="card-grid">
                <div class="card">
                    <h5>Precision: The Accuracy of Positive Predictions</h5>
                    <p><strong>Question it answers:</strong> "Out of all the times the model predicted 'Positive', how often was it correct?"</p>
                    <p><strong>Formula:</strong> `TP / (TP + FP)`</p>
                    <p><strong>Use when:</strong> The cost of a False Positive is high. For example, in email spam detection, you want high precision to avoid classifying important emails as spam.</p>
                </div>
                <div class="card">
                    <h5>Recall: The Ability to Find All Positives</h5>
                    <p><strong>Question it answers:</strong> "Out of all the actual 'Positive' cases, how many did the model find?"</p>
                    <p><strong>Formula:</strong> `TP / (TP + FN)`</p>
                    <p><strong>Use when:</strong> The cost of a False Negative is high. For example, in medical diagnoses for a serious disease, you want high recall to minimize the risk of missing a case.</p>
                </div>
            </div>
            
            <div class="highlight-box">
                <h3>The Precision-Recall Trade-off & The F1-Score</h3>
                <p>Often, increasing precision will decrease recall, and vice-versa. The <strong>F1-Score</strong> is the harmonic mean of Precision and Recall, providing a single metric that balances both. It's especially useful for imbalanced datasets where you care about finding the minority class.</p>
                <p><strong>Formula:</strong> `2 * (Precision * Recall) / (Precision + Recall)`</p>
            </div>
        </div>

        <!-- Page 10: Unsupervised Learning -->
        <div id="unsupervised" class="page">
            <h1>üìâ Unsupervised Learning & Dimensionality Reduction</h1>
            <p>Discovering patterns in data without predefined labels.</p>
            <div class="card-grid">
                <div class="card">
                    <h5>Clustering with K-Means</h5>
                    <p>K-Means is a popular clustering algorithm that aims to partition 'n' observations into 'k' clusters. It works by assigning each data point to the nearest cluster center (centroid) and then recalculating the centroids based on the new assignments, iterating until the assignments no longer change.</p>
                </div>
                <div class="card">
                    <h5>Principal Component Analysis (PCA)</h5>
                    <p>PCA is a technique for dimensionality reduction. It transforms a large set of correlated variables into a smaller set of uncorrelated variables called "principal components" while retaining most of the original information. The first principal component captures the most variance in the data, the second captures the next most, and so on.</p>
                </div>
            </div>
        </div>

        <!-- Page 11: Lab Exercise (k-NN) -->
        <div id="lab-exercise" class="page">
            <h1>Lab Exercise: Predicting Customer Churn with k-NN</h1>
            <h2>1. Learning Goals</h2>
            <ul>
                <li>Create and inspect a realistic, synthetic dataset.</li>
                <li>Perform essential data cleaning and preprocessing.</li>
                <li>Implement, train, and evaluate a k-Nearest Neighbors (k-NN) classification model.</li>
                <li>Interpret a confusion matrix and find the optimal value for 'k'.</li>
            </ul>
            <h2>2. Background</h2>
            <p><strong>The Problem:</strong> Predicting which customers are likely to churn (stop using a service) is a valuable task. Our goal is to predict churn based on features like account tenure and monthly bill.</p>
            <h2>3. Lab Procedure</h2>
            <h3>Step 1: Setup and Data Creation</h3>
            <div class="code-block"><pre>import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Generate synthetic customer data
np.random.seed(42)
n_samples = 500
tenure = np.random.randint(1, 73, n_samples)
monthly_charges = np.random.uniform(20, 120, n_samples)
churn = np.where((tenure < 12) & (monthly_charges > 70), 1, 0)
df = pd.DataFrame({'tenure': tenure, 'monthly_charges': monthly_charges, 'churn': churn})
</pre></div>
            <h3>Step 2: Preprocessing, Training, and Evaluation</h3>
            <div class="code-block"><pre>X = df[['tenure', 'monthly_charges']]
y = df['churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train_scaled, y_train)
y_pred = knn.predict(X_test_scaled)

print(classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
# plt.show() would be used here
</pre></div>
            <h2>4. Hyperparameter Tuning</h2>
            <p>Find the best value for 'k' by testing a range of options and plotting their accuracies.</p>
            <div class="code-block"><pre>k_range = range(1, 26)
accuracies = []
for k in k_range:
    knn_loop = KNeighborsClassifier(n_neighbors=k)
    knn_loop.fit(X_train_scaled, y_train)
    accuracies.append(accuracy_score(y_test, knn_loop.predict(X_test_scaled)))

plt.plot(k_range, accuracies, marker='o')
# plt.show() would be used here
</pre></div>
            <p><strong>Challenge:</strong> By observing the plot, what value of 'k' provides the highest accuracy?</p>
        </div>
        
        <!-- Page 12: Advanced Exercises -->
        <div id="advanced-exercise" class="page">
            <h1>Advanced Application Exercises & Solutions</h1>
            
            <h2>Challenge 1: Predictive Maintenance with Boosting</h2>
            <div class="card" style="margin-top: 2rem;">
                <p><strong>Scenario:</strong> A factory wants to predict equipment failure based on sensor data (temperature, vibration, pressure). The dataset is imbalanced, with few failure events.</p>
                <p><strong>Tasks:</strong> Build a `GradientBoostingClassifier`, tune it with `GridSearchCV`, and evaluate using precision, recall, and F1-score.</p>
            </div>
            <button class="accordion-button">Show/Hide Solution for Challenge 1</button>
            <div class="accordion-panel">
                <h3>Solution Walkthrough</h3>
                <p>First, we import libraries and create a synthetic imbalanced dataset where 'failure' (class 1) is rare.</p>
                <div class="code-block"><pre>from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.datasets import make_classification

# 1. Create an imbalanced dataset
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,
                           n_redundant=0, n_classes=2, n_clusters_per_class=1,
                           weights=[0.95, 0.05], flip_y=0, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
</pre></div>
                <p>Next, we define the parameter grid for `GridSearchCV` and run the search.</p>
                <div class="code-block"><pre># 2. Set up GridSearchCV
param_grid = {
    'n_estimators': [100, 150],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 4]
}
gb_clf = GradientBoostingClassifier(random_state=42)
grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1)
grid_search.fit(X_train, y_train)

print(f"Best Parameters: {grid_search.best_params_}")
</pre></div>
                <p>Finally, we evaluate the best model found by the grid search.</p>
                 <div class="code-block"><pre># 3. Evaluate the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['No Failure', 'Failure']))
</pre></div>
                <h3>Why Precision/Recall over Accuracy?</h3>
                <p>In an imbalanced dataset like this (e.g., 95% 'No Failure', 5% 'Failure'), a lazy model could predict 'No Failure' every time and achieve 95% accuracy. This model would be useless because it never identifies the events we care about. <strong>Precision</strong> and <strong>Recall</strong> for the 'Failure' class tell us how good the model is at its real job: identifying failures correctly without raising too many false alarms.</p>
            </div>

            <h2 style="margin-top: 3rem;">Challenge 2: Image Classification with a Neural Network</h2>
            <div class="card" style="margin-top: 2rem;">
                <p><strong>Scenario:</strong> Classify images from the CIFAR-10 dataset using a Convolutional Neural Network (CNN).</p>
                <p><strong>Tasks:</strong> Preprocess the data, build a CNN with TensorFlow/Keras, compile and train it, and analyze the training plots for overfitting.</p>
            </div>
            <button class="accordion-button">Show/Hide Solution for Challenge 2</button>
            <div class="accordion-panel">
                <h3>Solution Walkthrough</h3>
                <p>First, we load the CIFAR-10 dataset and preprocess it.</p>
                <div class="code-block"><pre>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# 1. Load and preprocess data
(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()

# Scale pixel values to be between 0 and 1
X_train = X_train.astype("float32") / 255.0
X_test = X_test.astype("float32") / 255.0

# One-hot encode the labels
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)
</pre></div>
                <p>Next, we define, compile, and train our CNN model.</p>
                <div class="code-block"><pre># 2. Build and compile the CNN model
model = keras.Sequential([
    keras.Input(shape=(32, 32, 3)),
    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(10, activation="softmax"),
])
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# 3. Train the model
history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_split=0.1)
</pre></div>
            <h3>Analyzing for Overfitting</h3>
            <p>We plot the training and validation accuracy/loss. <strong>Overfitting</strong> occurs when the model performs well on training data but poorly on validation data. We would see the training accuracy continuing to rise while validation accuracy flattens or drops. The gap between the training loss (decreasing) and validation loss (increasing) is the key indicator.</p>
            <div class="code-block"><pre># 4. Plot history to check for overfitting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy Over Epochs')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss Over Epochs')
# plt.show()
</pre></div>
            </div>
        </div>

    </main>

    <script>
        // Page switching logic
        function showPage(pageId, element) {
            const pages = document.querySelectorAll('.page');
            pages.forEach(page => page.classList.remove('active'));
            const activePage = document.getElementById(pageId);
            if (activePage) activePage.classList.add('active');
            
            const navLinks = document.querySelectorAll('.nav-links a');
            navLinks.forEach(link => link.classList.remove('active'));
            
            const correspondingLink = document.querySelector(`.nav-links a[onclick*="'${pageId}'"]`);
            if (correspondingLink) {
                correspondingLink.classList.add('active');
            } else if (element) {
                element.classList.add('active');
            }

            window.scrollTo({
                top: document.querySelector('main').offsetTop - 80,
                behavior: 'smooth'
            });
        }

        // Accordion logic
        const accordions = document.querySelectorAll('.accordion-button');
        accordions.forEach(acc => {
            acc.addEventListener("click", function() {
                this.classList.toggle("active");
                const panel = this.nextElementSibling;
                if (panel.style.maxHeight) {
                    panel.style.maxHeight = null;
                } else {
                    panel.style.maxHeight = panel.scrollHeight + "px";
                } 
            });
        });
    </script>
</body>
</html>
